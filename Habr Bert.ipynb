{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m288.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow_hub\n",
      "  Obtaining dependency information for tensorflow_hub from https://files.pythonhosted.org/packages/6e/1a/fbae76f4057b9bcdf9468025d7a8ca952dec14bfafb9fc0b1e4244ce212f/tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow_text\n",
      "  Obtaining dependency information for tensorflow_text from https://files.pythonhosted.org/packages/aa/76/2af69939d95bdf36fe5339d4c2bd7a5909d3e666c561cf1cf3d874b423c6/tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/89/18/0c6d563df29f3934495074ff62a9668e13c92cd588272773cf8c32407764/spacy-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading spacy-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting dask\n",
      "  Obtaining dependency information for dask from https://files.pythonhosted.org/packages/49/b9/5ed4a8ea2dc8731ae81fe6f815d90a59c75bb1ccaf9014351775c5d7850c/dask-2023.9.3-py3-none-any.whl.metadata\n",
      "  Downloading dask-2023.9.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/de/ce/b5d9c7ce1aaf9023b823c81932a50cd5e8f407198a696b0d1c6025a40b03/pandas-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/09/63/25e76075081ea98ec48f23929cefee58be0b42212e38074a9ec5c19e838c/tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pybind11>=2.2 (from fasttext)\n",
      "  Obtaining dependency information for pybind11>=2.2 from https://files.pythonhosted.org/packages/06/55/9f73c32dda93fa4f539fafa268f9504e83c489f460c380371d94296126cd/pybind11-2.11.1-py3-none-any.whl.metadata\n",
      "  Using cached pybind11-2.11.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from fasttext) (68.0.0)\n",
      "Collecting numpy (from fasttext)\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/c4/36/161e2f8110f8c49e59f6107bd6da4257d30aff9f06373d0471811f73dcc5/numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m564.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.19.6 (from tensorflow_hub)\n",
      "  Obtaining dependency information for protobuf>=3.19.6 from https://files.pythonhosted.org/packages/c8/2c/03046cac73f46bfe98fc846ef629cf4f84c2f59258216aa2cc0d22bfca8f/protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/93/1b/d880be7ac028cab6bf980acf005c16c0ff381f0c0ba1fd60c284626df3fd/murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/e5/bc/761acaf88b1fa69a6b75b55c24fbd8b47dab1a3c414d9512e907a646a048/cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/db/69/d9ab108dc670b5be9e292bbd555f39e6eb0a4baab25cd28f792850d5e65b/preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.1.8 from https://files.pythonhosted.org/packages/07/7d/f60693126441972336c094ff4eecbcb61d78ea6e92d2e4be4bf305bbe6e4/thinc-8.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading thinc-8.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/e2/a0/153375ade1ca9d33543da7d512329ea9a7d40dc0e0832599f4228b9d761b/srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/de/f5/6786a5fd1ab6a38511f3772c9002f312a2d509c1237ae514631adf145ad4/weasel-0.3.2-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m330.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0 (from spacy)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Obtaining dependency information for smart-open<7.0.0,>=5.2.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Obtaining dependency information for tqdm<5.0.0,>=4.38.0 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m300.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m432.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Obtaining dependency information for requests<3.0.0,>=2.13.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/73/66/0a72c9fcde42e5650c8d8d5c5c1873b9a3893018020c77ca8eb62708b923/pydantic-2.4.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m164.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m289.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting click>=8.0 (from dask)\n",
      "  Obtaining dependency information for click>=8.0 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle>=1.5.0 (from dask)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask)\n",
      "  Obtaining dependency information for fsspec>=2021.09.0 from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting partd>=1.2.0 (from dask)\n",
      "  Obtaining dependency information for partd>=1.2.0 from https://files.pythonhosted.org/packages/11/8a/b7a58e208b144a7315208a0dd627e23f5f50b47fa89c2924bb2e9238ecfb/partd-1.4.1-py3-none-any.whl.metadata\n",
      "  Downloading partd-1.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyyaml>=5.3.1 (from dask)\n",
      "  Obtaining dependency information for pyyaml>=5.3.1 from https://files.pythonhosted.org/packages/7b/5e/efd033ab7199a0b2044dab3b9f7a4f6670e6a52c089de572e928d2873b06/PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m212.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m173.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata>=4.13.0 (from dask)\n",
      "  Obtaining dependency information for importlib-metadata>=4.13.0 from https://files.pythonhosted.org/packages/cc/37/db7ba97e676af155f5fcb1a35466f446eadc9104e25b83366e8088c9c926/importlib_metadata-6.8.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m304.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m72.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m72.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/1e/e9/61d7338e503d63d2ce733373fa86256614f579b173cf3d0571d4f46cb561/h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/87/91/d57c2d22e4801edeb7f3e7939214c0ea8a28c6e16f85208c2df2145e0213/ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m180.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for wrapt<1.15,>=1.11.0 from https://files.pythonhosted.org/packages/7f/1b/e0439eec0db6520968c751bc7e12480bb80bb8d939190e0e55ed762f3c7a/wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/4c/64/245746084cdd5fafa680a6e7effeecf87abeeac2796decfa835a99b397c7/tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/e7/f9/33e17bb938d4b2afc7373120190e857f951d26f899992a9e717121170e2a/grpcio-1.59.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.59.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.13.0->dask)\n",
      "  Obtaining dependency information for zipp>=0.5 from https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl.metadata\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting locket (from partd>=1.2.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for pydantic-core==2.10.1 from https://files.pythonhosted.org/packages/39/09/120c06a52ed4bb1022d060bec0a16e5deb4ce79a1c4c11ef9519bc32b59f/pydantic_core-2.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/ff/b6/9222090f396f33cd58aa5b08b9bbf8871416b746a0c7b412a41a973674a5/charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/26/40/9957270221b6d3e9a3b92fdfba80dd5c9661ff45a664b47edd5d00f707f5/urllib3-2.0.6-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/b6/a5/54b01f663d60d5334f6c9c87c26274e94617a4fd463d812463626423b10d/werkzeug-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-3.0.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/dc/23/eb01450dc284a7ea8ebc0e5296f1f8fdbe5299169f4c318f836b4284a119/blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/93/f8/e89268a1f885048fb2ee6b5c9f93c4e90de768534acfef3652f87d97d4cb/confection-0.1.3-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.3-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cloudpathlib<0.16.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.16.0,>=0.7.0 from https://files.pythonhosted.org/packages/97/a2/e9a5bd762cccefc92a98c87354a65a8b75c280ab187a05e6d5851adbdae6/cloudpathlib-0.15.1-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m29.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m35.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m62.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m28.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m25.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m103.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m16.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:11\u001b[0mm\n",
      "\u001b[?25hDownloading dask-2023.9.3-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m94.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m181.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/489.9 MB\u001b[0m \u001b[31m9.7 kB/s\u001b[0m eta \u001b[36m8:30:38\u001b[0mm\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/http/client.py\", line 466, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/ssl.py\", line 1311, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/ssl.py\", line 1167, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 248, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 161, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 565, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 479, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/network/download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow_hub\n",
      "  Obtaining dependency information for tensorflow_hub from https://files.pythonhosted.org/packages/6e/1a/fbae76f4057b9bcdf9468025d7a8ca952dec14bfafb9fc0b1e4244ce212f/tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow_text\n",
      "  Obtaining dependency information for tensorflow_text from https://files.pythonhosted.org/packages/aa/76/2af69939d95bdf36fe5339d4c2bd7a5909d3e666c561cf1cf3d874b423c6/tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting numpy>=1.12.0 (from tensorflow_hub)\n",
      "  Obtaining dependency information for numpy>=1.12.0 from https://files.pythonhosted.org/packages/c4/36/161e2f8110f8c49e59f6107bd6da4257d30aff9f06373d0471811f73dcc5/numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting protobuf>=3.19.6 (from tensorflow_hub)\n",
      "  Obtaining dependency information for protobuf>=3.19.6 from https://files.pythonhosted.org/packages/c8/2c/03046cac73f46bfe98fc846ef629cf4f84c2f59258216aa2cc0d22bfca8f/protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow<2.15,>=2.14.0 (from tensorflow_text)\n",
      "  Obtaining dependency information for tensorflow<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/09/63/25e76075081ea98ec48f23929cefee58be0b42212e38074a9ec5c19e838c/tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/1e/e9/61d7338e503d63d2ce733373fa86256614f579b173cf3d0571d4f46cb561/h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/87/91/d57c2d22e4801edeb7f3e7939214c0ea8a28c6e16f85208c2df2145e0213/ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (23.1)\n",
      "Requirement already satisfied: setuptools in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (4.7.1)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for wrapt<1.15,>=1.11.0 from https://files.pythonhosted.org/packages/7f/1b/e0439eec0db6520968c751bc7e12480bb80bb8d939190e0e55ed762f3c7a/wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/4c/64/245746084cdd5fafa680a6e7effeecf87abeeac2796decfa835a99b397c7/tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/e7/f9/33e17bb938d4b2afc7373120190e857f951d26f899992a9e717121170e2a/grpcio-1.59.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached grpcio-1.59.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/b6/a5/54b01f663d60d5334f6c9c87c26274e94617a4fd463d812463626423b10d/werkzeug-3.0.0-py3-none-any.whl.metadata\n",
      "  Using cached werkzeug-3.0.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/ff/b6/9222090f396f33cd58aa5b08b9bbf8871416b746a0c7b412a41a973674a5/charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached charset_normalizer-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/26/40/9957270221b6d3e9a3b92fdfba80dd5c9661ff45a664b47edd5d00f707f5/urllib3-2.0.6-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.0.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/rebelraider/anaconda3/envs/Projects/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached tensorflow_text-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m103.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m22.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m20.8 kB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/489.9 MB\u001b[0m \u001b[31m28.9 kB/s\u001b[0m eta \u001b[36m4:30:34\u001b[0m^C\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/489.9 MB\u001b[0m \u001b[31m28.9 kB/s\u001b[0m eta \u001b[36m4:30:34\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fasttext tensorflow_hub tensorflow_text spacy dask pandas tensorflow fasttext spacy\n",
    "%pip install tensorflow_hub tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLayer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow_hub import KerasLayer\n",
    "import tensorflow_text as text\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import fasttext\n",
    "from html import unescape\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import spacy\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f2ab41e5d80>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x7f2ab41e5b40>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f2ac1edd380>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f2acbd51ac0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.ru.lemmatizer.RussianLemmatizer at 0x7f2acbdd9080>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f2acb69e3b0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель 'ru_core_news_sm' уже установлена.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Инициализируйте SpaCy и установите стоп-слова\n",
    "try:\n",
    "    nlp = spacy.load(\"ru_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "    print(\"Модель 'ru_core_news_sm' уже установлена.\")\n",
    "except OSError:\n",
    "    print(\"Модель 'ru_core_news_sm' не установлена. Скачиваем...\")\n",
    "    spacy.cli.download(\"ru_core_news_sm\")\n",
    "    nlp = spacy.load(\"ru_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "STOPWORDS = nlp.Defaults.stop_words\n",
    "PARTS_OF_SPEECH_TO_REMOVE = [\"ADP\", \"NUM\", \"DET\", \"PRON\", \"CONJ\", \"PART\", \"INTJ\"]\n",
    "PUNCTUATION_REGEX = re.compile(r' ([,\\.])')\n",
    "HTML_TAG_PATTERN = re.compile(r'<[^>]+>')\n",
    "LINK_PATTERN = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "SPECIAL_CHAR_PATTERN = re.compile(r'[^\\w\\s.,\\!\\?]')\n",
    "MULTIPLE_SPACES_PATTERN = re.compile(r'\\s+')\n",
    "LANGUAGE_MODEL = fasttext.load_model('/habr/lid.176.bin')\n",
    "CALL_LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "def timing_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if not hasattr(wrapper, 'call_count'):\n",
    "            wrapper.call_count = 0\n",
    "            wrapper.start_time = time.time()\n",
    "\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        wrapper.call_count += 1\n",
    "        if wrapper.call_count % CALL_LIMIT == 0:\n",
    "            elapsed_time = time.time() - wrapper.start_time\n",
    "            print(f\"Вызовов: {wrapper.call_count}, Время выполнения последних {CALL_LIMIT} вызовов функции {func.__name__}: {elapsed_time} секунд\")\n",
    "            wrapper.start_time = time.time()\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timing_decorator\n",
    "def clear_text(text):\n",
    "    # Объединение операций регулярных выражений в одну\n",
    "    text = unescape(HTML_TAG_PATTERN.sub('', text))  # Удаление HTML-тегов\n",
    "    text = LINK_PATTERN.sub('', text) # Удаление ссылок\n",
    "    text = SPECIAL_CHAR_PATTERN.sub(' ', text)  # Замена специальных символов\n",
    "    text = MULTIPLE_SPACES_PATTERN.sub(' ', text)  # Замена множественных пробелов на один\n",
    "    text = text.strip()  # Удаление начальных и конечных пробелов\n",
    "    return text\n",
    "\n",
    "\n",
    "@timing_decorator\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join(\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.pos_ not in PARTS_OF_SPEECH_TO_REMOVE and token.lemma_ not in STOPWORDS\n",
    "    )\n",
    "    return PUNCTUATION_REGEX.sub(r'\\1', lemmatized_text)\n",
    "\n",
    "\n",
    "def category_preprocessing(categories, num_words):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(categories)\n",
    "    category_sequences = tokenizer.texts_to_sequences(categories)\n",
    "    max_sequence_length = len(max(category_sequences, key=len))\n",
    "    padded_category_sequences = tf.keras.preprocessing.sequence.pad_sequences(category_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "    return padded_category_sequences, max_sequence_length, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "def habr_model(hubs_length, input_hubs_dim, tags_length, input_tags_dim, output_hubs_dim=64, output_tags_dim=64,\n",
    "               dense_1_units=128, dense_2_units=64, dropout_1_proc=0.2, dropout_2_proc=0.2):\n",
    "    \"\"\"\n",
    "    Создает модель для задачи многомерной регрессии, объединяющую текстовую информацию с категориальными и числовыми признаками.\n",
    "\n",
    "    Параметры:\n",
    "        hubs_length (int): Длина входных категориальных признаков \"hubs\".\n",
    "        input_hubs_dim (int): Размерность входных категориальных признаков \"hubs\".\n",
    "        tags_length (int): Длина входных категориальных признаков \"tags\".\n",
    "        input_tags_dim (int): Размерность входных категориальных признаков \"tags\".\n",
    "        output_hubs_dim (int, по умолчанию 64): Размерность эмбеддингов для категориальных признаков \"hubs\".\n",
    "        output_tags_dim (int, по умолчанию 64): Размерность эмбеддингов для категориальных признаков \"tags\".\n",
    "        dense_1_units (int, по умолчанию 128): Количество нейронов в первом полносвязном слое.\n",
    "        dense_2_units (int, по умолчанию 64): Количество нейронов во втором полносвязном слое.\n",
    "        dropout_1_proc (float, по умолчанию 0.2): Вероятность \"выключения\" нейронов в первом Dropout слое.\n",
    "        dropout_2_proc (float, по умолчанию 0.2): Вероятность \"выключения\" нейронов во втором Dropout слое.\n",
    "\n",
    "    Возвращает:\n",
    "        tf.keras.Model: Скомпилированную модель для задачи многомерной регрессии.\n",
    "\n",
    "    Примечания:\n",
    "        - Для текстовых данных используется BERT для предварительной обработки и извлечения признаков.\n",
    "        - Категориальные признаки \"hubs\" и \"tags\" преобразуются с помощью Embedding слоев.\n",
    "        - Все признаки (текстовые, категориальные и числовые) конкатенируются в один тензор.\n",
    "        - Добавлены два полносвязных слоя с функцией активации ReLU и Dropout слои для регуляризации.\n",
    "        - Выходной слой без функции активации предсказывает два регрессионных значения.\n",
    "\n",
    "    Пример использования:\n",
    "        model = habr_model(hubs_length=10, input_hubs_dim=100, tags_length=20, input_tags_dim=200)\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    \"\"\"\n",
    "    bert_preprocess = KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "    bert_encoder = KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\", trainable=True)\n",
    "    \n",
    "    # Входной слой для числового признака \"publicationTime\"\n",
    "    input_publicationTime = tf.keras.layers.Input(shape=(1,), name='publicationTime')\n",
    "    \n",
    "    # Определение входных слоев для категориальных признаков \"tags\" и \"hubs\"\n",
    "    input_tags = tf.keras.layers.Input(shape=(tags_length,), name='tags_input')\n",
    "    input_hubs = tf.keras.layers.Input(shape=(hubs_length,), name='hubs_input')\n",
    "    \n",
    "    # Входные слои для текстовых признаков \"title\" и \"text\"\n",
    "    input_title = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"title_input\")\n",
    "    input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    \n",
    "    # Предварительная обработка текстовых данных с помощью BERT\n",
    "    preprocessed_text = bert_preprocess(input_text)\n",
    "    encoded_text = bert_encoder(preprocessed_text)\n",
    "    preprocessed_title = bert_preprocess(input_title)\n",
    "    encoded_title = bert_encoder(preprocessed_title)\n",
    "    \n",
    "    # Эмбеддинги для категориальных признаков \"hubs\" и \"tags\"\n",
    "    embedded_hubs = tf.keras.layers.Embedding(input_hubs_dim, output_hubs_dim, input_length=hubs_length, name='hubs_embedding')(input_hubs)\n",
    "    embedded_tags = tf.keras.layers.Embedding(input_tags_dim, output_tags_dim, input_length=tags_length, name=\"tags_embedding\")(input_tags)\n",
    "    \n",
    "    # Конкатенация всех признаков\n",
    "    concatenated_features = tf.keras.layers.Concatenate()([encoded_text, encoded_title, embedded_hubs, embedded_tags, input_publicationTime])\n",
    "    \n",
    "    # Первый полносвязный слой с активацией ReLU\n",
    "    dense_layer_1 = tf.keras.layers.Dense(dense_1_units, activation='relu')(concatenated_features)\n",
    "    \n",
    "    # Dropout слой для регуляризации\n",
    "    dropout_1 = tf.keras.layers.Dropout(dropout_1_proc)(dense_layer_1)\n",
    "    \n",
    "    # Второй полносвязный слой с активацией ReLU\n",
    "    dense_layer_2 = tf.keras.layers.Dense(dense_2_units, activation='relu')(dropout_1)\n",
    "    \n",
    "    # Dropout слой для регуляризации\n",
    "    dropout_2 = tf.keras.layers.Dropout(dropout_2_proc)(dense_layer_2)\n",
    "    \n",
    "    # Выходной слой для регрессии\n",
    "    out = tf.keras.layers.Dense(2, name=\"output\")(dropout_2)\n",
    "    \n",
    "    # Создание и возвращение модели\n",
    "    model = tf.keras.Model(inputs=[input_text, input_title, input_tags, input_hubs, input_publicationTime], outputs=[out])\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publicationTime</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "      <th>hubs</th>\n",
       "      <th>rating</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>665066</td>\n",
       "      <td>0</td>\n",
       "      <td>Обзор Harvester — гиперконвергентного Open Sou...</td>\n",
       "      <td>Немецкая компания SUSE известна в Open Source-...</td>\n",
       "      <td>[гиперконвергенция, kubernetes, kubevirt, long...</td>\n",
       "      <td>[Блог компании Флант, Open source, Виртуализац...</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>659559</td>\n",
       "      <td>0</td>\n",
       "      <td>Функциональные тесты на проекте: жизнь до и после</td>\n",
       "      <td>Наша команда отвечает за продажи в Skyeng, лич...</td>\n",
       "      <td>[функциональные тесты, test, php, codeception,...</td>\n",
       "      <td>[Блог компании Skyeng, Тестирование IT-систем,...</td>\n",
       "      <td>4</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>665004</td>\n",
       "      <td>0</td>\n",
       "      <td>Как и для чего мы построили ML Space</td>\n",
       "      <td>Речь пойдет о платформе для ML-разработки полн...</td>\n",
       "      <td>[SberCloud, ML Space, облачные вычисления, маш...</td>\n",
       "      <td>[Блог компании SberCloud, Облачные вычисления,...</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>665300</td>\n",
       "      <td>0</td>\n",
       "      <td>Сегментация экземпляров с помощью Mask R-CNN</td>\n",
       "      <td>Задача сегментации изображений может решаться ...</td>\n",
       "      <td>[mask r-cnn, аугментации, semantic segmentatio...</td>\n",
       "      <td>[Машинное обучение, Программирование, Python]</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>665204</td>\n",
       "      <td>0</td>\n",
       "      <td>Письмо счастья: как мы разделили наши сборки д...</td>\n",
       "      <td>В марте Google Play стал рассылать письмо-пред...</td>\n",
       "      <td>[google play, appgallery, huawei mobile servic...</td>\n",
       "      <td>[Блог компании HeadHunter, Разработка мобильны...</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110874</th>\n",
       "      <td>426</td>\n",
       "      <td>5680</td>\n",
       "      <td>Firefox 2 вышел.</td>\n",
       "      <td>Между прочим, втихушу firefox 2 закачали на ft...</td>\n",
       "      <td>[firefox, браузеры]</td>\n",
       "      <td>[Firefox]</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110875</th>\n",
       "      <td>407</td>\n",
       "      <td>5683</td>\n",
       "      <td>MyNews.webalta.ru</td>\n",
       "      <td>Вебальта задумала сделать большой-большой Хабр...</td>\n",
       "      <td>[вебальта, хабрахабр, webalta]</td>\n",
       "      <td>[Поисковые технологии]</td>\n",
       "      <td>6</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110876</th>\n",
       "      <td>4536</td>\n",
       "      <td>5686</td>\n",
       "      <td>«Яндекс» снова занялся интернет-математикой</td>\n",
       "      <td>«Яндекс» во второй раз проводит конкурс научны...</td>\n",
       "      <td>[Яндекс, Интернет-математика-2007, наука, поис...</td>\n",
       "      <td>[Математика]</td>\n",
       "      <td>0</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110877</th>\n",
       "      <td>380</td>\n",
       "      <td>5686</td>\n",
       "      <td>Зёрна от плевел…</td>\n",
       "      <td>Многие ли из Вас ощущают давление информационн...</td>\n",
       "      <td>[новости, RSS, спам]</td>\n",
       "      <td>[Информационная безопасность]</td>\n",
       "      <td>5</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110878</th>\n",
       "      <td>379</td>\n",
       "      <td>5686</td>\n",
       "      <td>Третий релиз-кандидат ОгнеЛиса уже с нами!</td>\n",
       "      <td>Уже вышел FF_2_RC3 — и это радует.\\n\\r\\nНадеюс...</td>\n",
       "      <td>[firefox]</td>\n",
       "      <td>[Firefox]</td>\n",
       "      <td>2</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110879 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  publicationTime  \\\n",
       "0       665066                0   \n",
       "1       659559                0   \n",
       "2       665004                0   \n",
       "3       665300                0   \n",
       "4       665204                0   \n",
       "...        ...              ...   \n",
       "110874     426             5680   \n",
       "110875     407             5683   \n",
       "110876    4536             5686   \n",
       "110877     380             5686   \n",
       "110878     379             5686   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Обзор Harvester — гиперконвергентного Open Sou...   \n",
       "1       Функциональные тесты на проекте: жизнь до и после   \n",
       "2                    Как и для чего мы построили ML Space   \n",
       "3            Сегментация экземпляров с помощью Mask R-CNN   \n",
       "4       Письмо счастья: как мы разделили наши сборки д...   \n",
       "...                                                   ...   \n",
       "110874                                   Firefox 2 вышел.   \n",
       "110875                                  MyNews.webalta.ru   \n",
       "110876        «Яндекс» снова занялся интернет-математикой   \n",
       "110877                                   Зёрна от плевел…   \n",
       "110878         Третий релиз-кандидат ОгнеЛиса уже с нами!   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Немецкая компания SUSE известна в Open Source-...   \n",
       "1       Наша команда отвечает за продажи в Skyeng, лич...   \n",
       "2       Речь пойдет о платформе для ML-разработки полн...   \n",
       "3       Задача сегментации изображений может решаться ...   \n",
       "4       В марте Google Play стал рассылать письмо-пред...   \n",
       "...                                                   ...   \n",
       "110874  Между прочим, втихушу firefox 2 закачали на ft...   \n",
       "110875  Вебальта задумала сделать большой-большой Хабр...   \n",
       "110876  «Яндекс» во второй раз проводит конкурс научны...   \n",
       "110877  Многие ли из Вас ощущают давление информационн...   \n",
       "110878  Уже вышел FF_2_RC3 — и это радует.\\n\\r\\nНадеюс...   \n",
       "\n",
       "                                                     tags  \\\n",
       "0       [гиперконвергенция, kubernetes, kubevirt, long...   \n",
       "1       [функциональные тесты, test, php, codeception,...   \n",
       "2       [SberCloud, ML Space, облачные вычисления, маш...   \n",
       "3       [mask r-cnn, аугментации, semantic segmentatio...   \n",
       "4       [google play, appgallery, huawei mobile servic...   \n",
       "...                                                   ...   \n",
       "110874                                [firefox, браузеры]   \n",
       "110875                     [вебальта, хабрахабр, webalta]   \n",
       "110876  [Яндекс, Интернет-математика-2007, наука, поис...   \n",
       "110877                               [новости, RSS, спам]   \n",
       "110878                                          [firefox]   \n",
       "\n",
       "                                                     hubs  rating  views  \n",
       "0       [Блог компании Флант, Open source, Виртуализац...       0    213  \n",
       "1       [Блог компании Skyeng, Тестирование IT-систем,...       4    129  \n",
       "2       [Блог компании SberCloud, Облачные вычисления,...       0     68  \n",
       "3           [Машинное обучение, Программирование, Python]       0     77  \n",
       "4       [Блог компании HeadHunter, Разработка мобильны...       0    579  \n",
       "...                                                   ...     ...    ...  \n",
       "110874                                          [Firefox]       4   1500  \n",
       "110875                             [Поисковые технологии]       6    854  \n",
       "110876                                       [Математика]       0    795  \n",
       "110877                      [Информационная безопасность]       5    931  \n",
       "110878                                          [Firefox]       2    623  \n",
       "\n",
       "[110879 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Habr = pd.read_json('/habr/habr.json')\n",
    "Habr.drop(\"author\", axis=1, inplace=True)\n",
    "Habr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rebelraider/Python projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb Ячейка 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Habr[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Habr[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(clear_text)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m Habr[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Habr[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(lemmatize_text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Habr[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Habr[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(clear_text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Habr[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Habr[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(lemmatize_text)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/pandas/core/series.py:4539\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\n\u001b[1;32m   4461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4462\u001b[0m     arg: Callable \u001b[39m|\u001b[39m Mapping \u001b[39m|\u001b[39m Series,\n\u001b[1;32m   4463\u001b[0m     na_action: Literal[\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   4464\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m   4465\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4466\u001b[0m \u001b[39m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4467\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4537\u001b[0m \u001b[39m    dtype: object\u001b[39;00m\n\u001b[1;32m   4538\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4539\u001b[0m     new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_values(arg, na_action\u001b[39m=\u001b[39;49mna_action)\n\u001b[1;32m   4540\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_values, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\u001b[39m.\u001b[39m__finalize__(\n\u001b[1;32m   4541\u001b[0m         \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4542\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/pandas/core/base.py:890\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    889\u001b[0m \u001b[39m# mapper is a function\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m new_values \u001b[39m=\u001b[39m map_f(values, mapper)\n\u001b[1;32m    892\u001b[0m \u001b[39mreturn\u001b[39;00m new_values\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/home/rebelraider/Python projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb Ячейка 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m doc \u001b[39m=\u001b[39m Doc(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m doc\u001b[39m.\u001b[39msegment(segmenter)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m doc\u001b[39m.\u001b[39;49mtag_morph(morph_tagger)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m doc\u001b[39m.\u001b[39mtag_ner(ner_tagger)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39mtokens:\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/natasha/doc.py:136\u001b[0m, in \u001b[0;36mDoc.tag_morph\u001b[0;34m(self, tagger)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtag_morph\u001b[39m(\u001b[39mself\u001b[39m, tagger):\n\u001b[0;32m--> 136\u001b[0m     tag_morph_doc(\u001b[39mself\u001b[39;49m, tagger)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/natasha/doc.py:212\u001b[0m, in \u001b[0;36mtag_morph_doc\u001b[0;34m(doc, tagger)\u001b[0m\n\u001b[1;32m    210\u001b[0m chunk \u001b[39m=\u001b[39m [sent_words(_) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n\u001b[1;32m    211\u001b[0m markups \u001b[39m=\u001b[39m tagger\u001b[39m.\u001b[39mmap(chunk)\n\u001b[0;32m--> 212\u001b[0m \u001b[39mfor\u001b[39;00m sent, markup \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(doc\u001b[39m.\u001b[39msents, markups):\n\u001b[1;32m    213\u001b[0m     inject_morph(sent\u001b[39m.\u001b[39mtokens, markup\u001b[39m.\u001b[39mtokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/natasha/morph/tagger.py:75\u001b[0m, in \u001b[0;36mMorphTagger.map\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[1;32m     74\u001b[0m     markups \u001b[39m=\u001b[39m SlovnetMorph\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m, items)\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mfor\u001b[39;00m markup \u001b[39min\u001b[39;00m markups:\n\u001b[1;32m     76\u001b[0m         \u001b[39myield\u001b[39;00m adapt_markup(markup)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/slovnet/api.py:35\u001b[0m, in \u001b[0;36mAPI.map\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[1;32m     34\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chop(items, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(chunk)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/slovnet/exec/infer.py:66\u001b[0m, in \u001b[0;36mMorphInfer.__call__\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     63\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess(inputs)\n\u001b[1;32m     64\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(preds)\n\u001b[0;32m---> 66\u001b[0m \u001b[39mfor\u001b[39;00m item, pred \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(items, preds):\n\u001b[1;32m     67\u001b[0m     tuples \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(item, pred)\n\u001b[1;32m     68\u001b[0m     \u001b[39myield\u001b[39;00m MorphMarkup\u001b[39m.\u001b[39mfrom_tuples(tuples)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/slovnet/exec/infer.py:28\u001b[0m, in \u001b[0;36mTagDecoder.__call__\u001b[0;34m(self, preds)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, preds):\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m preds:\n\u001b[1;32m     29\u001b[0m         \u001b[39myield\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags_vocab\u001b[39m.\u001b[39mdecode(_) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m pred]\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/slovnet/exec/infer.py:57\u001b[0m, in \u001b[0;36mMorphInfer.process\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m     56\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m inputs:\n\u001b[0;32m---> 57\u001b[0m         pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mword_id, \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mshape_id, \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mpad_mask)\n\u001b[1;32m     58\u001b[0m         pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mhead\u001b[39m.\u001b[39mdecode(pred)\n\u001b[1;32m     59\u001b[0m         \u001b[39myield from\u001b[39;00m split_masked(pred, \u001b[39m~\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mpad_mask)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/slovnet/exec/model.py:326\u001b[0m, in \u001b[0;36mTag.__call__\u001b[0;34m(self, word_id, shape_id, pad_mask)\u001b[0m\n\u001b[1;32m    324\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(word_id, shape_id)\n\u001b[1;32m    325\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x, pad_mask)\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/slovnet/exec/model.py:314\u001b[0m, in \u001b[0;36mMorphHead.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Projects/lib/python3.10/site-packages/slovnet/exec/model.py:65\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     63\u001b[0m shape \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\n\u001b[1;32m     64\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_dim)\n\u001b[0;32m---> 65\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49marray) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39marray\n\u001b[1;32m     67\u001b[0m shape \u001b[39m=\u001b[39m shape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_dim,)\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    Habr['text'] = list(executor.map(process_text, Habr['text']))\n",
    "    Habr['title'] = list(executor.map(process_text, Habr['title']))\n",
    "#без Executor: более 100 минут (далее не имело смысла проверять)\n",
    "#с Executor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вызовов: 200, Время выполнения последних 100 вызовов функции process_text: 69.1024980545044 секунд\n",
      "Вызовов: 100, Время выполнения последних 100 вызовов функции process_text: 64.19351553916931 секунд\n",
      "Вызовов: 300, Время выполнения последних 100 вызовов функции process_text: 66.58453798294067 секунд\n",
      "Вызовов: 200, Время выполнения последних 100 вызовов функции process_text: 60.98379135131836 секунд\n",
      "Вызовов: 400, Время выполнения последних 100 вызовов функции process_text: 74.36879348754883 секунд\n",
      "Вызовов: 300, Время выполнения последних 100 вызовов функции process_text: 64.20496892929077 секунд\n",
      "Вызовов: 500, Время выполнения последних 100 вызовов функции process_text: 62.112890005111694 секунд\n",
      "Вызовов: 400, Время выполнения последних 100 вызовов функции process_text: 74.62389183044434 секунд\n",
      "Вызовов: 600, Время выполнения последних 100 вызовов функции process_text: 65.14502930641174 секунд\n",
      "Вызовов: 500, Время выполнения последних 100 вызовов функции process_text: 63.30695295333862 секунд\n",
      "Вызовов: 700, Время выполнения последних 100 вызовов функции process_text: 64.65649580955505 секунд\n",
      "Вызовов: 600, Время выполнения последних 100 вызовов функции process_text: 63.612993240356445 секунд\n",
      "Вызовов: 800, Время выполнения последних 100 вызовов функции process_text: 68.63043022155762 секунд\n",
      "Вызовов: 700, Время выполнения последних 100 вызовов функции process_text: 62.27061343193054 секунд\n",
      "Вызовов: 900, Время выполнения последних 100 вызовов функции process_text: 58.285643577575684 секунд\n",
      "Вызовов: 800, Время выполнения последних 100 вызовов функции process_text: 70.22475337982178 секунд\n",
      "Вызовов: 1000, Время выполнения последних 100 вызовов функции process_text: 67.40376329421997 секунд\n",
      "Вызовов: 900, Время выполнения последних 100 вызовов функции process_text: 57.32843208312988 секунд\n",
      "Вызовов: 1100, Время выполнения последних 100 вызовов функции process_text: 63.59747672080994 секунд\n",
      "Вызовов: 1000, Время выполнения последних 100 вызовов функции process_text: 65.25227737426758 секунд\n",
      "Вызовов: 1200, Время выполнения последних 100 вызовов функции process_text: 60.670055866241455 секунд\n",
      "Вызовов: 1100, Время выполнения последних 100 вызовов функции process_text: 63.048728466033936 секунд\n",
      "Вызовов: 1300, Время выполнения последних 100 вызовов функции process_text: 73.7128791809082 секунд\n",
      "Вызовов: 1200, Время выполнения последних 100 вызовов функции process_text: 62.608673095703125 секунд\n",
      "Вызовов: 1400, Время выполнения последних 100 вызовов функции process_text: 56.81119084358215 секунд\n",
      "Вызовов: 1300, Время выполнения последних 100 вызовов функции process_text: 71.94770789146423 секунд\n",
      "Вызовов: 1500, Время выполнения последних 100 вызовов функции process_text: 71.32737398147583 секунд\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rebelraider/Python projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb Ячейка 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ddf \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39mfrom_pandas(Habr, npartitions\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ddf[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ddf[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(process_text, meta\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rebelraider/Python%20projects/MachineLearning/ForVSUET/HabrPredict/Habr_BERT.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m Habr \u001b[39m=\u001b[39m ddf\u001b[39m.\u001b[39;49mcompute()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/base.py:381\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    358\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/base.py:666\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    664\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 666\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    667\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddf = dd.from_pandas(Habr, npartitions=8)\n",
    "ddf['text'] = ddf['text'].map(process_text, meta=('text', 'str'))\n",
    "Habr = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "languages_count = defaultdict(int)\n",
    "for text in Habr['text']:\n",
    "    lang = LANGUAGE_MODEL.predict(text)[0][0].split(\"__label__\")[1]\n",
    "    languages_count[lang] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for lang, count in languages_count.items():\n",
    "    display(f\"Язык: {lang}, Количество: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "len(Habr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(Habr)):\n",
    "    lang = LANGUAGE_MODEL.predict(Habr['text'][i])[0][0].split(\"__label__\")[1]\n",
    "    if lang != \"ru\":\n",
    "        Habr.drop(i, inplace=True)\n",
    "Habr.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "len(Habr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Counter([hub for row in Habr[\"hubs\"] for hub in row]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "num_words_hubs = 700\n",
    "hubs, hubs_length, hubs_tokenizer = category_preprocessing(Habr[\"hubs\"], num_words=num_words_hubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "display(hubs)\n",
    "display(hubs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Counter([tag for row in Habr[\"tags\"] for tag in row]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "num_words_tags = 1000\n",
    "tags, tags_length, tags_tokenizer = category_preprocessing(Habr[\"tags\"], num_words=num_words_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "display(tags)\n",
    "display(tags_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "time_scaler = StandardScaler()\n",
    "publicationTime = time_scaler.fit_transform(Habr['publicationTime'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rating_Scaler = StandardScaler()\n",
    "rating = rating_Scaler.fit_transform(Habr['rating'].values.reshape(-1, 1))\n",
    "\n",
    "views_Scaler = StandardScaler()\n",
    "views = views_Scaler.fit_transform(Habr['views'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить лемматизацию при помощи Наташи\n",
    "рассортировать проект\n",
    "Выпросить вычислительные мощности для запуска\n",
    "Оптимизировать гиперпараметры\n",
    "проверить tfidf категорий через text_to_matrix\n",
    "Наделать кучу графиков и визуальных штук\n",
    "поискать еще фичи"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
